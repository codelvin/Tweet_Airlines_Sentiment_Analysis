{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> Before this line are method predefined</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):  \n",
    "    \"\"\" \n",
    "    Tokenization/string cleaning for dataset \n",
    "    Every dataset is lower cased except \n",
    "    \"\"\"  \n",
    "    sens = word_tokenize(string.lower())\n",
    "    sens = [word for word in sens if not word in english_stopwords]\n",
    "    sens = [word for word in sens if not word in english_punctuations]\n",
    "    sens = [lemmatizer.lemmatize(word) for word in sens]\n",
    "    sens = [word for word in sens if word.isalpha()]\n",
    "    sens = ' '.join(sens)\n",
    "    return sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(X, label):\n",
    "    # only for this case!\n",
    "    plt.figure()\n",
    "    # plt.scatter(aa[:,0],aa[:,1])\n",
    "    point_1 = []\n",
    "    point_0 = []\n",
    "    point_2 = []\n",
    "    for i in range(len(label)):\n",
    "        if label[i]== '1' or label[i]== 1:\n",
    "            point_1.append(X[i])\n",
    "        elif label[i]== '0' or label[i]== 0:\n",
    "            point_0.append(X[i])\n",
    "        else:\n",
    "            point_2.append(X[i])\n",
    "    point_1 = np.asarray(point_1)\n",
    "    point_0 = np.asarray(point_0)\n",
    "    point_2 = np.asarray(point_2)\n",
    "    plt.scatter(point_1[:,0],point_1[:,1],color='red')\n",
    "    plt.scatter(point_0[:,0],point_0[:,1],color='g')\n",
    "    plt.scatter(point_2[:,0],point_2[:,1],color='b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_tsne(X, label):\n",
    "    ts = TSNE()\n",
    "    X_lower = ts.fit_transform(X.reshape(X.shape[0], X.shape[2]),)\n",
    "    plot_2d(X_lower, label)\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> Now we do some preprocessing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.02\n",
    "MAX_FEATURES = 2000\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "DECAY = 2e-4  # about half each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['airline_sentiment']\n",
    "y.replace({'neutral':'2', 'positive':'1', 'negative':'0'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9178\n",
       "2    3099\n",
       "1    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocssing, stopwords and rare words, tokenization and vectorizing\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_punctuations = [',', '.','\\'s', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "X = X.apply(clean_str)\n",
    "\n",
    "# vectorizing using tfidf\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2), max_df = 0.95,min_df = 0.001, max_features = MAX_FEATURES)\n",
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:red\"> For our first model, bidirectional LSTM with fine-tuning </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11712, 1, 1618)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 517)\n",
    "X_train = X_train.toarray()\n",
    "X_train = np.reshape(X_train,(X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.toarray()\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "bidirectional_1_input (InputLay (None, 1, 1618)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 1618)      0           bidirectional_1_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 1618)      0           bidirectional_1_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 3)            1789699     lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Concatenate)           (None, 3)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,789,699\n",
      "Trainable params: 1,789,699\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bi_lstm = Sequential()\n",
    "#bi_lstm.add(Dense(512, activation = 'relu', input_shape = (1, X_train.shape[2])))\n",
    "#bi_lstm.add(Dropout(0.3))\n",
    "bi_lstm.add(Bidirectional(LSTM(128, recurrent_dropout = 0.3, return_sequences=False),input_shape = (1, X_train.shape[2])))\n",
    "#bi_lstm.add(Dense(64, activation = 'relu'))\n",
    "bi_lstm.add(Dense(3,activation = 'softmax'))\n",
    "bi_lstm = multi_gpu_model(bi_lstm)\n",
    "bi_lstm.summary()\n",
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9369 samples, validate on 2343 samples\n",
      "Epoch 1/100\n",
      " - 26s - loss: 0.8912 - acc: 0.6314 - val_loss: 0.8736 - val_acc: 0.6116\n",
      "Epoch 2/100\n",
      " - 20s - loss: 0.8199 - acc: 0.6348 - val_loss: 0.8076 - val_acc: 0.6287\n",
      "Epoch 3/100\n",
      " - 20s - loss: 0.7561 - acc: 0.6651 - val_loss: 0.7534 - val_acc: 0.6633\n",
      "Epoch 4/100\n",
      " - 20s - loss: 0.7033 - acc: 0.7058 - val_loss: 0.7097 - val_acc: 0.6957\n",
      "Epoch 5/100\n",
      " - 20s - loss: 0.6612 - acc: 0.7315 - val_loss: 0.6810 - val_acc: 0.7089\n",
      "Epoch 6/100\n",
      " - 20s - loss: 0.6287 - acc: 0.7436 - val_loss: 0.6547 - val_acc: 0.7243\n",
      "Epoch 7/100\n",
      " - 20s - loss: 0.6024 - acc: 0.7503 - val_loss: 0.6367 - val_acc: 0.7294\n",
      "Epoch 8/100\n",
      " - 20s - loss: 0.5813 - acc: 0.7595 - val_loss: 0.6245 - val_acc: 0.7367\n",
      "Epoch 9/100\n",
      " - 20s - loss: 0.5624 - acc: 0.7654 - val_loss: 0.6128 - val_acc: 0.7439\n",
      "Epoch 10/100\n",
      " - 20s - loss: 0.5461 - acc: 0.7733 - val_loss: 0.6059 - val_acc: 0.7486\n",
      "Epoch 11/100\n",
      " - 20s - loss: 0.5319 - acc: 0.7814 - val_loss: 0.6004 - val_acc: 0.7482\n",
      "Epoch 12/100\n",
      " - 20s - loss: 0.5186 - acc: 0.7869 - val_loss: 0.5926 - val_acc: 0.7542\n",
      "Epoch 13/100\n",
      " - 20s - loss: 0.5064 - acc: 0.7938 - val_loss: 0.5896 - val_acc: 0.7576\n",
      "Epoch 14/100\n",
      " - 20s - loss: 0.4954 - acc: 0.8018 - val_loss: 0.5861 - val_acc: 0.7614\n",
      "Epoch 15/100\n",
      " - 20s - loss: 0.4854 - acc: 0.8070 - val_loss: 0.5822 - val_acc: 0.7644\n",
      "Epoch 16/100\n",
      " - 19s - loss: 0.4758 - acc: 0.8118 - val_loss: 0.5818 - val_acc: 0.7712\n",
      "Epoch 17/100\n",
      " - 20s - loss: 0.4674 - acc: 0.8150 - val_loss: 0.5789 - val_acc: 0.7708\n",
      "Epoch 18/100\n",
      " - 20s - loss: 0.4596 - acc: 0.8180 - val_loss: 0.5756 - val_acc: 0.7738\n",
      "Epoch 19/100\n",
      " - 20s - loss: 0.4521 - acc: 0.8250 - val_loss: 0.5791 - val_acc: 0.7708\n",
      "Epoch 20/100\n",
      " - 19s - loss: 0.4451 - acc: 0.8268 - val_loss: 0.5726 - val_acc: 0.7725\n",
      "Epoch 21/100\n",
      " - 20s - loss: 0.4384 - acc: 0.8299 - val_loss: 0.5734 - val_acc: 0.7691\n",
      "Epoch 22/100\n",
      " - 20s - loss: 0.4322 - acc: 0.8309 - val_loss: 0.5743 - val_acc: 0.7755\n",
      "Epoch 23/100\n",
      " - 20s - loss: 0.4261 - acc: 0.8345 - val_loss: 0.5748 - val_acc: 0.7695\n",
      "Epoch 24/100\n",
      " - 20s - loss: 0.4214 - acc: 0.8350 - val_loss: 0.5731 - val_acc: 0.7738\n",
      "Epoch 25/100\n",
      " - 20s - loss: 0.4156 - acc: 0.8381 - val_loss: 0.5745 - val_acc: 0.7742\n",
      "Epoch 26/100\n",
      " - 20s - loss: 0.4106 - acc: 0.8379 - val_loss: 0.5784 - val_acc: 0.7700\n",
      "Epoch 27/100\n",
      " - 20s - loss: 0.4058 - acc: 0.8420 - val_loss: 0.5755 - val_acc: 0.7751\n",
      "Epoch 28/100\n",
      " - 20s - loss: 0.4011 - acc: 0.8421 - val_loss: 0.5818 - val_acc: 0.7644\n",
      "Epoch 29/100\n",
      " - 20s - loss: 0.3972 - acc: 0.8429 - val_loss: 0.5804 - val_acc: 0.7734\n",
      "Epoch 30/100\n",
      " - 20s - loss: 0.3934 - acc: 0.8480 - val_loss: 0.5831 - val_acc: 0.7746\n",
      "Epoch 31/100\n",
      " - 20s - loss: 0.3895 - acc: 0.8494 - val_loss: 0.5833 - val_acc: 0.7738\n",
      "Epoch 32/100\n",
      " - 20s - loss: 0.3853 - acc: 0.8494 - val_loss: 0.5847 - val_acc: 0.7759\n",
      "Epoch 33/100\n",
      " - 20s - loss: 0.3820 - acc: 0.8500 - val_loss: 0.5866 - val_acc: 0.7712\n",
      "Epoch 34/100\n",
      " - 20s - loss: 0.3784 - acc: 0.8514 - val_loss: 0.5884 - val_acc: 0.7721\n",
      "Epoch 35/100\n",
      " - 20s - loss: 0.3755 - acc: 0.8540 - val_loss: 0.5904 - val_acc: 0.7738\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f36dc0e5320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr = LEARNING_RATE), metrics=['accuracy'])\n",
    "bi_lstm.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2, callbacks = [earlystopping], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928/2928 [==============================] - 1s 197us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5990455647309622, 0.7667349726775956]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:red\">SVM with Tf-Idf!</span>\n",
    "###  Use train and test from above. We are doing based on One-hot embedding method. We will try to do GloVe later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train and test from above. We are doing based on TFIDF embedding method. We will try to do GloVe later.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 517)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1, 10, 100], 'gamma': [0.1, 1, 10], 'kernel': ('linear', 'rbf')},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, grid_search\n",
    "\n",
    "# parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10,100], 'gamma':[0.1,1,10]}\n",
    "# svr = svm.SVC(class_weight = 'balanced', verbose = True)\n",
    "# clf = grid_search.GridSearchCV(svr, parameters)\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.764344262295082"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel = 'rbf',gamma = 0.1, C = 10,class_weight = 'balanced', verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Using GloVe pretrained word vector to do embedding!<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.02\n",
    "MAX_SEQUENCE_LENGTH = 120\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "DECAY = 5e-5  # about half each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (14640,),  y shape:(14640,)\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "y = df['airline_sentiment']\n",
    "y.replace({'neutral':'2', 'positive':'1', 'negative':'0'}, inplace = True)\n",
    "print(\"X shape: {},  y shape:{}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (14640,),  y shape:(14640,)\n"
     ]
    }
   ],
   "source": [
    "# preprocssing, stopwords and rare words, tokenization and vectorizing\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "english_punctuations = [',', '.','\\'s', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "X = X.apply(clean_str)\n",
    "print(\"X shape: {},  y shape:{}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10411 unique tokens.\n",
      "Shape of data tensor: (14640, 120)\n"
     ]
    }
   ],
   "source": [
    "# After getting rid of stopwords\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors\n"
     ]
    }
   ],
   "source": [
    "# OK, let's do embedding\n",
    "embedding_index = {}\n",
    "with open ('../data/glove.twitter.27B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "        embedding_index[word] = coefs\n",
    "        \n",
    "print('Found %s word vectors' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then embedding!\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11712, 120)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data,y,test_size = 0.2, random_state = 517)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=3)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=3)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedding_layer_input (InputLay (None, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 120)          0           embedding_layer_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 120)          0           embedding_layer_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 3)            1276467     lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Concatenate)          (None, 3)            0           sequential_19[1][0]              \n",
      "                                                                 sequential_19[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,276,467\n",
      "Trainable params: 235,267\n",
      "Non-trainable params: 1,041,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bi_lstm_glo = Sequential()\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            name='embedding_layer')\n",
    "bi_lstm_glo.add(embedding_layer)\n",
    "bi_lstm_glo.add(Dropout(0.2))\n",
    "bi_lstm_glo.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "#bi_lstm.add(Dense(64, activation = 'relu'))\n",
    "bi_lstm_glo.add(Dense(3,activation = 'softmax'))\n",
    "\n",
    "bi_lstm_glo = multi_gpu_model(bi_lstm_glo)\n",
    "bi_lstm_glo.summary()\n",
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9369 samples, validate on 2343 samples\n",
      "Epoch 1/20\n",
      "9369/9369 [==============================] - 72s 8ms/step - loss: 0.8640 - acc: 0.6329 - val_loss: 0.8422 - val_acc: 0.6146\n",
      "Epoch 2/20\n",
      "9369/9369 [==============================] - 63s 7ms/step - loss: 0.7957 - acc: 0.6583 - val_loss: 0.7712 - val_acc: 0.6739\n",
      "Epoch 3/20\n",
      "9369/9369 [==============================] - 64s 7ms/step - loss: 0.7299 - acc: 0.7065 - val_loss: 0.7067 - val_acc: 0.7093\n",
      "Epoch 4/20\n",
      "9369/9369 [==============================] - 62s 7ms/step - loss: 0.6717 - acc: 0.7370 - val_loss: 0.6539 - val_acc: 0.7418\n",
      "Epoch 5/20\n",
      "9369/9369 [==============================] - 62s 7ms/step - loss: 0.6415 - acc: 0.7499 - val_loss: 0.6291 - val_acc: 0.7482\n",
      "Epoch 6/20\n",
      "9369/9369 [==============================] - 63s 7ms/step - loss: 0.6275 - acc: 0.7544 - val_loss: 0.6155 - val_acc: 0.7623\n",
      "Epoch 7/20\n",
      "9369/9369 [==============================] - 63s 7ms/step - loss: 0.6148 - acc: 0.7575 - val_loss: 0.6221 - val_acc: 0.7409\n",
      "Epoch 8/20\n",
      "9369/9369 [==============================] - 63s 7ms/step - loss: 0.5995 - acc: 0.7588 - val_loss: 0.5885 - val_acc: 0.7610\n",
      "Epoch 9/20\n",
      "9369/9369 [==============================] - 62s 7ms/step - loss: 0.5874 - acc: 0.7638 - val_loss: 0.5860 - val_acc: 0.7691\n",
      "Epoch 10/20\n",
      "9369/9369 [==============================] - 62s 7ms/step - loss: 0.5862 - acc: 0.7651 - val_loss: 0.5846 - val_acc: 0.7589\n",
      "Epoch 11/20\n",
      "9369/9369 [==============================] - 60s 6ms/step - loss: 0.5732 - acc: 0.7672 - val_loss: 0.5917 - val_acc: 0.7589\n",
      "Epoch 12/20\n",
      "9369/9369 [==============================] - 61s 7ms/step - loss: 0.5701 - acc: 0.7690 - val_loss: 0.5747 - val_acc: 0.7636\n",
      "Epoch 13/20\n",
      "9369/9369 [==============================] - 62s 7ms/step - loss: 0.5623 - acc: 0.7704 - val_loss: 0.5773 - val_acc: 0.7674\n",
      "Epoch 14/20\n",
      "9369/9369 [==============================] - 59s 6ms/step - loss: 0.5596 - acc: 0.7666 - val_loss: 0.5749 - val_acc: 0.7678\n",
      "Epoch 15/20\n",
      "9369/9369 [==============================] - 61s 7ms/step - loss: 0.5616 - acc: 0.7714 - val_loss: 0.5775 - val_acc: 0.7700\n",
      "Epoch 16/20\n",
      "9369/9369 [==============================] - 62s 7ms/step - loss: 0.5581 - acc: 0.7721 - val_loss: 0.6244 - val_acc: 0.7482\n",
      "Epoch 17/20\n",
      "9369/9369 [==============================] - 60s 6ms/step - loss: 0.5541 - acc: 0.7760 - val_loss: 0.6320 - val_acc: 0.7431\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3550e55cf8>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "bi_lstm_glo.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr = LEARNING_RATE), metrics=['accuracy'])\n",
    "bi_lstm_glo.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2, callbacks = [earlystopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928/2928 [==============================] - 10s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6512906798899499, 0.742827868852459]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm_glo.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Using GloVe pretrained word vector to make a Feedforward neural net!<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedding_layer_input (InputLay (None, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 120)          0           embedding_layer_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 120)          0           embedding_layer_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "model_15 (Model)                (None, 3)            1276467     lambda_31[0][0]                  \n",
      "                                                                 lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Concatenate)          (None, 3)            0           model_15[1][0]                   \n",
      "                                                                 model_15[2][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,276,467\n",
      "Trainable params: 235,267\n",
      "Non-trainable params: 1,041,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp = Sequential()\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            name='embedding_layer')\n",
    "mlp.add(embedding_layer)\n",
    "mlp.add(Dropout(0.3))\n",
    "mlp.add(Flatten())\n",
    "mlp.add(Dense(128))\n",
    "mlp.add(Dropout(0.3))\n",
    "mlp.add(Dense(16))\n",
    "mlp.add(Dense(3,activation = 'softmax'))\n",
    "\n",
    "mlp = multi_gpu_model(bi_lstm_glo)\n",
    "mlp.summary()\n",
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 15, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9369 samples, validate on 2343 samples\n",
      "Epoch 1/20\n",
      "9369/9369 [==============================] - 136s 14ms/step - loss: 0.5533 - acc: 0.7724 - val_loss: 0.5595 - val_acc: 0.7751\n",
      "Epoch 2/20\n",
      "9369/9369 [==============================] - 124s 13ms/step - loss: 0.5479 - acc: 0.7754 - val_loss: 0.5656 - val_acc: 0.7764\n",
      "Epoch 3/20\n",
      "9369/9369 [==============================] - 123s 13ms/step - loss: 0.5503 - acc: 0.7752 - val_loss: 0.5550 - val_acc: 0.7734\n",
      "Epoch 4/20\n",
      "9369/9369 [==============================] - 122s 13ms/step - loss: 0.5480 - acc: 0.7723 - val_loss: 0.5606 - val_acc: 0.7721\n",
      "Epoch 5/20\n",
      "9369/9369 [==============================] - 122s 13ms/step - loss: 0.5458 - acc: 0.7723 - val_loss: 0.5641 - val_acc: 0.7721\n",
      "Epoch 6/20\n",
      "9369/9369 [==============================] - 123s 13ms/step - loss: 0.5458 - acc: 0.7772 - val_loss: 0.5580 - val_acc: 0.7729\n",
      "Epoch 7/20\n",
      "9369/9369 [==============================] - 125s 13ms/step - loss: 0.5422 - acc: 0.7745 - val_loss: 0.5780 - val_acc: 0.7687\n",
      "Epoch 8/20\n",
      "9369/9369 [==============================] - 125s 13ms/step - loss: 0.5414 - acc: 0.7832 - val_loss: 0.5781 - val_acc: 0.7648\n",
      "Epoch 9/20\n",
      "9369/9369 [==============================] - 123s 13ms/step - loss: 0.5403 - acc: 0.7741 - val_loss: 0.5823 - val_acc: 0.7700\n",
      "Epoch 10/20\n",
      "9369/9369 [==============================] - 124s 13ms/step - loss: 0.5395 - acc: 0.7795 - val_loss: 0.5543 - val_acc: 0.7682\n",
      "Epoch 11/20\n",
      "9369/9369 [==============================] - 124s 13ms/step - loss: 0.5371 - acc: 0.7752 - val_loss: 0.5566 - val_acc: 0.7708\n",
      "Epoch 12/20\n",
      "9369/9369 [==============================] - 123s 13ms/step - loss: 0.5385 - acc: 0.7752 - val_loss: 0.5664 - val_acc: 0.7687\n",
      "Epoch 13/20\n",
      "9369/9369 [==============================] - 122s 13ms/step - loss: 0.5356 - acc: 0.7807 - val_loss: 0.5641 - val_acc: 0.7725\n",
      "Epoch 14/20\n",
      "9369/9369 [==============================] - 123s 13ms/step - loss: 0.5336 - acc: 0.7814 - val_loss: 0.5489 - val_acc: 0.7738\n",
      "Epoch 15/20\n",
      "9369/9369 [==============================] - 123s 13ms/step - loss: 0.5315 - acc: 0.7824 - val_loss: 0.5581 - val_acc: 0.7708\n",
      "Epoch 16/20\n",
      "9369/9369 [==============================] - 123s 13ms/step - loss: 0.5376 - acc: 0.7767 - val_loss: 0.5462 - val_acc: 0.7751\n",
      "Epoch 17/20\n",
      "9369/9369 [==============================] - 125s 13ms/step - loss: 0.5281 - acc: 0.7799 - val_loss: 0.5634 - val_acc: 0.7653\n",
      "Epoch 18/20\n",
      "9369/9369 [==============================] - 124s 13ms/step - loss: 0.5272 - acc: 0.7844 - val_loss: 0.5618 - val_acc: 0.7708\n",
      "Epoch 19/20\n",
      "9369/9369 [==============================] - 124s 13ms/step - loss: 0.5277 - acc: 0.7831 - val_loss: 0.5545 - val_acc: 0.7721\n",
      "Epoch 20/20\n",
      "9369/9369 [==============================] - 123s 13ms/step - loss: 0.5309 - acc: 0.7808 - val_loss: 0.5553 - val_acc: 0.7704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f354985c3c8>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "mlp.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr = LEARNING_RATE), metrics=['accuracy'])\n",
    "mlp.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2, callbacks = [earlystopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928/2928 [==============================] - 20s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.573737268565131, 0.7684426229508197]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
